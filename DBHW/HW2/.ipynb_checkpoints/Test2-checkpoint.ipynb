{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math \n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(Path):\n",
    "    with open(Path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design write search log function \n",
    "def write_search_log(num_of_search, search_query, search_result):\n",
    "    search_path = dirpath + \"/search.txt\"\n",
    "    with open(search_path,'a') as f:\n",
    "        f.write(\"search {0}\\n\".format(num_of_search))\n",
    "        f.write(\"query {0}\\n\".format(search_query))\n",
    "        f.write(search_result+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_schedules(Path):\n",
    "    queries = []\n",
    "    schedules = []\n",
    "    with open(Path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "    #         print(line)\n",
    "            queries.append(line)\n",
    "            if(line[0] == \"<\"):\n",
    "        #         print(query.split(\"> \")[1])\n",
    "                schedules.append(line.split(\"> \")[1])\n",
    "            else:\n",
    "        #         print(query)\n",
    "                schedules.append(line)\n",
    "    return schedules, queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design write search log function \n",
    "def write_transaction_log(dirpath, SQL):\n",
    "    # parse transaction\n",
    "    log = \"\"\n",
    "    if SQL[5:5+6] == \"commit\":\n",
    "        log = \"<{0}> commit\\n\".format(re.split(\">\",SQL.split(\"<\")[1])[0])\n",
    "#         print(log)\n",
    "    elif SQL[5:5+6] == \"DELETE\":\n",
    "        [tran, command, table, key_value] = re.split(\"> |FROM |WHERE \",SQL.split(\"<\")[1]) \n",
    "        [key, key_v, _] = re.split(\" = |;\",key_value)\n",
    "        log = \"<{0}>, <{1}>.<{2}>, <{3}>, \\n\".format(tran,table,key,key_v)\n",
    "#         print(log)\n",
    "    elif SQL[5:5+6] == \"UPDATE\":    \n",
    "        [tran, command_table, col_value, key_value] = re.split(\"> |SET |WHERE \",SQL.split(\"<\")[1])\n",
    "        [command, table, _] = command_table.split(\" \")\n",
    "        [col, col_v] = re.split(\" = \",col_value)\n",
    "        [key, key_v, _] = re.split(\" = |;\",key_value)\n",
    "        # col_oldv = get_old_value\n",
    "        with con.cursor() as cursor:\n",
    "            cursor.execute(\"select {0} from {1} where {2} = {3}\".format(col, table, key, key_v))\n",
    "            old_v = cursor.fetchall()[0][0]\n",
    "#             print(old_v[0][0])\n",
    "        log = \"<{0}>, <{1}>.<{2}>.<{3}>, <{4}>, <{5}>\\n\".format(tran,table,key_v,col,old_v,col_v )\n",
    "#         print(log)\n",
    "    elif SQL[5:5+8] == \"rollback\":\n",
    "        log = \"<{0}> abort\\n\".format(re.split(\">\",SQL.split(\"<\")[1])[0])\n",
    "#         print(log)\n",
    "    log_path = dirpath + \"/prj2.log\"\n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_checkpoint(dirpath, active_set):\n",
    "    log_path = dirpath + \"/prj2.log\"    \n",
    "    log = \"checkpoint \"\n",
    "    print(\"checkpoint \",end=\"\")\n",
    "    i = 0\n",
    "    for tran in sorted(active_transactions):\n",
    "        if( i != len(active_transactions) - 1):\n",
    "            print(\"<{0}>, \".format(tran),end=\"\")\n",
    "            log +=  \"<{0}>, \".format(tran)\n",
    "        else:\n",
    "            print(\"<\",tran,\">\")\n",
    "            log +=  \"<\"+tran+\">\\n\"\n",
    "        i += 1\n",
    "    if(len(active_transactions) == 0):\n",
    "        log += \"\\n\"\n",
    "    \n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_based_recovery(dirpath, line_number, active_transactions, line_last_checkpoint):\n",
    "    log_path = dirpath + \"/recovery.txt\"\n",
    "    schedule_path = dirpath + \"/prj2.sched\"\n",
    "    header = \"recover <{0}>\\n\".format(line_number)\n",
    "    print(header)\n",
    "#     print(line_last_checkpoint)\n",
    "#     print(active_transactions)\n",
    "    \n",
    "    Undo = active_transactions.copy()\n",
    "    Redo = set()\n",
    "    \n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(header)\n",
    "    \n",
    "    count = 1\n",
    "    print(\"====================================\")\n",
    "    with open(schedule_path,'r') as f:\n",
    "        while True:\n",
    "            if count in range(line_last_checkpoint, line_number):\n",
    "                line = f.readline()\n",
    "                print(line)\n",
    "            if count == line_number:\n",
    "                break\n",
    "            count += 1 \n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFscore(Nd, Ndt, Nt):\n",
    "        return math.log1p(Ndt/Nd)*(1/Nt)\n",
    "    \n",
    "def SortCriteria(item):\n",
    "        return item[2]*item[3]\n",
    "       \n",
    "def print_search_query(querys, R):\n",
    "    TFIDFs_idx = {}\n",
    "    TFIDFs_idx_inverse = {}\n",
    "    TFIDFs = []\n",
    "    iteration = 0\n",
    "    for query in querys:\n",
    "        #######################################################################\n",
    "        # # TF-IDF Score calculation for each word \n",
    "        #######################################################################\n",
    "        sql = \"select sum(freq),id from InvertedIndex where id in (select id from InvertedIndex where term = %s) group by id order by id\"\n",
    "        cursor.execute(sql,query)\n",
    "        NdInfo = cursor.fetchall()\n",
    "\n",
    "        sql = \"select freq, id, term from InvertedIndex where term = %s order by id\"\n",
    "        cursor.execute(sql,query)\n",
    "        NdtInfo = cursor.fetchall()\n",
    "\n",
    "        sql = \"select count(*) from InvertedIndex where term = %s\"\n",
    "        cursor.execute(sql,query)\n",
    "        Nt = cursor.fetchall()[0][0]\n",
    "\n",
    "        TFIDF = []\n",
    "        for i in range(len(NdtInfo)):\n",
    "            if NdInfo[i][1] == NdtInfo[i][1]:\n",
    "                Nd = NdInfo[i][0]\n",
    "                Ndt = NdtInfo[i][0]\n",
    "                id = NdInfo[i][1]\n",
    "        #         print(id,\" \",math.log1p(Ndt/Nd)*(1/Nt))\n",
    "        #         TFIDF.append((id, math.log1p(Ndt/Nd)*(1/Nt)))\n",
    "                TFIDF.append((id, TFIDFscore(Nd, Ndt, Nt)))\n",
    "        TFIDFs.append(TFIDF)\n",
    "        TFIDFs_idx[query] = iteration\n",
    "        TFIDFs_idx_inverse[iteration] = query\n",
    "        iteration = iteration + 1\n",
    "\n",
    "    # Get title dictionary     \n",
    "    sql = \"select id,title from wiki order by id\"\n",
    "    cursor.execute(sql)\n",
    "    id_title = cursor.fetchall()\n",
    "    id_title_dictionary = {}\n",
    "    for _, idtitle in enumerate(id_title):\n",
    "        v_id = idtitle[0]\n",
    "        v_title = idtitle[1]\n",
    "        id_title_dictionary[v_id] = v_title\n",
    "\n",
    "    # Union TFIDFs\n",
    "    TFIDFsetlist = []      \n",
    "    for query in querys:\n",
    "        temp = np.array(TFIDFs[TFIDFs_idx[query]])\n",
    "        if len(temp) == 0:\n",
    "            idsForOneQuery = temp.astype(int)\n",
    "        else:\n",
    "            idsForOneQuery = temp[...,0].astype(int)\n",
    "        TFIDFsetlist.append(set(idsForOneQuery))\n",
    "\n",
    "    UnionId = []\n",
    "    UnionId_idx = {}\n",
    "    UnionId_idx_inverse = {}\n",
    "    for TFIDFset in TFIDFsetlist:\n",
    "        UnionId = list(set(UnionId)|set(TFIDFset))\n",
    "\n",
    "    UnionId.sort()\n",
    "    iteration = 0;\n",
    "    for idvalue in UnionId:\n",
    "        UnionId_idx[idvalue] = iteration\n",
    "        UnionId_idx_inverse[iteration] = idvalue\n",
    "        iteration = iteration + 1\n",
    "\n",
    "    # Get TFIDF Score Matrix SM\n",
    "    SM = np.zeros((len(querys), len(UnionId)))\n",
    "    for row in range(len(querys)):\n",
    "        LenOfcols = len(TFIDFs[row])\n",
    "    #     print(LenOfcols)\n",
    "        for j in range(LenOfcols): # j means each query's index of id\n",
    "    #         print(TFIDFs[row][j][0], TFIDFs[0][j][1], TFIDFs[0][j][2])  # \n",
    "            col = UnionId_idx[TFIDFs[row][j][0]]\n",
    "    #         title = TFIDFs[row][j][1]\n",
    "            score = TFIDFs[row][j][1]\n",
    "            SM[row][col] = score\n",
    "    # SM\n",
    "    #######################################################################\n",
    "    # # Get Top K list\n",
    "    # # Input: TFIDF Score Matrix SM, PageRank list R\n",
    "    # # Output: QAList(sorted order by TFIDF score * PageRank score)\n",
    "    #######################################################################\n",
    "    QAList = []\n",
    "    for it in UnionId_idx:\n",
    "        Id = it\n",
    "        UnionTFIDFScore = SM[...,UnionId_idx[it]].sum()\n",
    "        Uniontitle = id_title_dictionary[it] \n",
    "        PrankScore = R[N_idx[Id]][0]\n",
    "        QAList.append((Id, Uniontitle, UnionTFIDFScore, PrankScore))\n",
    "\n",
    "    QAList.sort(key= SortCriteria, reverse= True)\n",
    "\n",
    "    strFormat = '%-10s%-60s%-20s%-20s\\n'\n",
    "    strOut = strFormat % ('id', 'title', 'TF-IDF', 'PageRank')\n",
    "    iteration = 0;\n",
    "    for ans in QAList:\n",
    "        if iteration < 10:\n",
    "#             print(ans[0], ans[1], format(ans[2],\"10.2e\"), format(ans[3],\"10.2e\"))\n",
    "            strOut += strFormat %(ans[0], ans[1], format(ans[2],\"10.2e\"), format(ans[3],\"10.2e\"))\n",
    "        else:\n",
    "           break\n",
    "        iteration = iteration + 1\n",
    "    print(strOut)\n",
    "    \n",
    "    return strOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing\n",
    "* create inverted index\n",
    "* calculate pagerank score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# # Connect to mysql\n",
    "# # Output: con obj, cursor obj\n",
    "#######################################################################\n",
    "# IP = \"s.snu.ac.kr\"\n",
    "# ID = \"ADB2018_26190\"\n",
    "# PW = \"ADB2018_26190\"\n",
    "# DBase = \"ADB2018_26190\"\n",
    "PW = \"1360\"\n",
    "DBASE = \"test\"\n",
    "WIKI = \"wiki\"\n",
    "LINK = \"link\"\n",
    "\n",
    "con = pymysql.connect(host = 'localhost', \n",
    "                            user = 'root',\n",
    "                            password = PW,\n",
    "                            db = DBASE,\n",
    "                            charset = 'utf8mb4')\n",
    "cursor = con.cursor()\n",
    "dirpath = os.getcwd()\n",
    "\n",
    "print(\"building tables...\")\n",
    "# # Before executing this file, create wiki, link table in DBase using mysql-workbench  \n",
    "#######################################################################\n",
    "# # Make inverted index table \n",
    "# # Input: wiki table \n",
    "# # Output: InvertedIndex table(term, id, title, freq)\n",
    "#######################################################################\n",
    "sql = \"select * from wiki\"\n",
    "cursor.execute(sql)\n",
    "result_wiki = cursor.fetchall()\n",
    "\n",
    "sql = \"select count(*) from information_schema.tables where (table_schema = %s) and (table_name = %s)\"\n",
    "cursor.execute(sql, (DBASE, \"InvertedIndex\"))\n",
    "\n",
    "# if inverted index table does not exist, create inverted index table \n",
    "if cursor.fetchall()[0][0] == 0: \n",
    "    # Create inverted index table \n",
    "    sql = \"create table InvertedIndex (term varchar(255) not null, id int(11) not null, freq int(11) not null)\"\n",
    "    cursor.execute(sql)\n",
    "    # # Clean inverted index table\n",
    "    # sql = \"delete from InvertedIndex\" \n",
    "    # cursor.execute(sql)\n",
    "\n",
    "    # insertion from wiki table \n",
    "    sql = \"insert into InvertedIndex (term,id,freq) values (%s,%s,%s)\"\n",
    "    for Doc in result_wiki:\n",
    "    #     print(type(Doc)) #tuple (id, doc_name, doc_script)\n",
    "        tokens = nltk.word_tokenize(Doc[2].lower())\n",
    "        fdist = nltk.FreqDist(tokens) # dictionary {term:freq, ... }\n",
    "        for term, freq in fdist.items():\n",
    "            cursor.execute(sql,(term, Doc[0], freq))\n",
    "    #     print(fdist)\n",
    "    con.commit()\n",
    "    \n",
    "#cursor.execute(\"drop table InvertedIndex\")\n",
    "\n",
    "print(\"ready to search...\")\n",
    "#######################################################################\n",
    "# # PageRank Score Calculation\n",
    "# # Input: link table \n",
    "# # Output: PageRank list R(id, PageRank score)\n",
    "#######################################################################\n",
    "# print(\"calculating pagerank score ...\")\n",
    "# Get Ni for each\n",
    "sql = \"select id_from, count(*) as outgoing from link group by id_from order by id_from\"\n",
    "cursor.execute(sql)\n",
    "FromNiInfo = cursor.fetchall()\n",
    "\n",
    "Ni_dict = {}\n",
    "for idx, FromNi in enumerate(FromNiInfo):\n",
    "#     print(FromNi[0], FromNi[1])\n",
    "    id_from = FromNi[0]\n",
    "    Ni = FromNi[1]\n",
    "    Ni_dict[id_from] = Ni\n",
    "\n",
    "# Get N and id_all(sorted order) \n",
    "sql = \"select distinct id_from from link order by id_from\"\n",
    "cursor.execute(sql)\n",
    "SetOfFromInfo = cursor.fetchall()\n",
    "\n",
    "sql = \"select distinct id_to from link order by id_to\"\n",
    "cursor.execute(sql)\n",
    "SetOfToInfo = cursor.fetchall()\n",
    "\n",
    "SetOfFrom = set()\n",
    "SetOfTo = set()\n",
    "\n",
    "for idx, From in enumerate(SetOfFromInfo):\n",
    "#     print(type(From[0]))\n",
    "    SetOfFrom.add(From[0])\n",
    "    \n",
    "for idx, To in enumerate(SetOfToInfo):\n",
    "#     print(type(From[0]))\n",
    "    SetOfTo.add(To[0])\n",
    "    \n",
    "id_all = sorted(SetOfFrom.union(SetOfTo))\n",
    "N = len(id_all)\n",
    "\n",
    "# Get N_idx and N_idx_inverse; it means dictionary[Doc.id] = index of Transition Matrix or State Matrix    \n",
    "N_idx = {}\n",
    "N_idx_inverse = {}\n",
    "for idInfo, idx in enumerate(id_all):\n",
    "#     print(idx, idInfo)\n",
    "    N_idx[idx] = idInfo\n",
    "    N_idx_inverse[idInfo] = idx\n",
    "\n",
    "# Get SateMatrix S; check whether existing from j to i link  \n",
    "S = np.zeros((N,N)) # from j to i info : S[i][j]\n",
    "sql = \"select * from link order by id_from\"\n",
    "cursor.execute(sql)\n",
    "FromToInfo = np.array(cursor.fetchall())\n",
    "for fromto in FromToInfo:\n",
    "    id_to = fromto[1]\n",
    "    id_from = fromto[0]\n",
    "    S[N_idx[id_to]][N_idx[id_from]] = 1\n",
    "\n",
    "# Get Transition Matrix M and Score Vector R \n",
    "M = np.zeros((N,N)) # from j to i info : M[i][j]\n",
    "for _, id_from in enumerate(sorted(SetOfFrom)):\n",
    "    for _, id_to in enumerate(id_all):\n",
    "        # if link id_from to id_to exists\n",
    "        if S[N_idx[id_to]][N_idx[id_from]] != 0:\n",
    "            M[N_idx[id_to]][N_idx[id_from]] = 1/Ni_dict[id_from]\n",
    "\n",
    "# PageRank Algorithm\n",
    "# Input: Station Matrix S, Transition Matrix T, RankVector R \n",
    "# Output: updated RankVector R \n",
    "delta = 0.15\n",
    "elipslion = 1e-8\n",
    "# R = np.ones((N,1))*(1/N)\n",
    "R = np.ones((N,1))\n",
    "K = np.ones((N,1))*(delta/N)\n",
    "# R = delta * np.matmul(M,prevR) + K\n",
    "iteration = 0\n",
    "distance = 100\n",
    "while distance > elipslion:\n",
    "#     print(\"iteration\", iteration, \"...\")\n",
    "    prevR = R\n",
    "    R = delta * np.matmul(M,R) + K\n",
    "    iteration = iteration + 1\n",
    "    distance = np.linalg.norm(R-prevR)\n",
    "#     print(\"distance = \",np.linalg.norm(R-prevR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# # TF-IDF Score calculation\n",
    "# # Input: InvertedIndex table, wiki table, query(string)\n",
    "# # Output: TF-IDF list TFIDF(id, title, TF-IDF score) \n",
    "#######################################################################\n",
    "while True:\n",
    "    # Get input from console\n",
    "    print(\"2018-26190>\",end = '')\n",
    "    query = input()\n",
    "    if query == \"exit()\":\n",
    "        break\n",
    "    elif query[0] == \"-\": # -run *.sched # -run prj2.sched\n",
    "        schedules_path = dirpath + \"/\"+query.split(\"-run \")[1]\n",
    "        if os.path.isfile(schedules_path):\n",
    "            [schedules, queries] = parse_schedules(schedules_path)\n",
    "            # initialize search.txt\n",
    "            num_of_search = 0\n",
    "            open(dirpath+'/search.txt', 'w').close()\n",
    "            open(dirpath+'/prj2.log', 'w').close()\n",
    "            open(dirpath+'/recovery.txt', 'w').close()\n",
    "            active_transactions = set()\n",
    "            line_number = 1\n",
    "            line_last_checkpoint = 1\n",
    "            \n",
    "            # execute schedules =================== 6 functions should be implemented \n",
    "            for schedule in queries:\n",
    "                if schedule[:6] == \"search\":\n",
    "                    search_query = schedule.split(\"search \")[1]\n",
    "                    print(search_query)\n",
    "#                     print(\"execute search\")\n",
    "                    search_result = print_search_query(search_query.lower().split(), R)\n",
    "                    write_search_log(num_of_search, search_query, search_result)\n",
    "                    num_of_search += 1   \n",
    "                elif schedule[0] == \"<\":\n",
    "                    # active_transaction update\n",
    "                    if schedule[1:3] not in active_transactions:\n",
    "                        active_transactions.add(schedule[1:3])\n",
    "                        print(\"<{0}> start\\n\".format(schedule[1:3]))\n",
    "                        with open(dirpath+\"/prj2.log\",'a') as f:\n",
    "                            f.write(\"<{0}> start\\n\".format(schedule[1:3]))\n",
    "                    elif schedule[5:5+6] == \"commit\":\n",
    "                        active_transactions.remove(schedule[1:3])\n",
    "                    elif schedule[5:5+8] == \"rollback\":\n",
    "                        active_transactions.remove(schedule[1:3])\n",
    "                    print(schedule)\n",
    "                    write_transaction_log(dirpath, schedule)\n",
    "                elif schedule[:10] == \"checkpoint\":\n",
    "                    print(schedule)\n",
    "                    write_checkpoint(dirpath, sorted(active_transactions))\n",
    "                    line_last_checkpoint = line_number\n",
    "                elif schedule[:6] == \"system\":\n",
    "                    print(schedule)\n",
    "                    log_based_recovery(dirpath, line_number, active_transactions, line_last_checkpoint)\n",
    "                line_number += 1\n",
    "        else:\n",
    "            print(\"such file is not exist\")\n",
    "    else:\n",
    "        querys = query.lower().split()\n",
    "        print_search_query(querys, R)\n",
    "    \n",
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_based_recovery(dirpath, line_number, active_transactions, line_last_checkpoint):\n",
    "    log_path = dirpath + \"/recovery.txt\"\n",
    "    schedule_path = dirpath + \"/prj2.sched\"\n",
    "    \n",
    "    header = \"recover {0}\\n\".format(line_number)\n",
    "    print(header)\n",
    "    print(\"last check point line: \",line_last_checkpoint)\n",
    "    print(active_transactions)\n",
    "    \n",
    "    Undo = active_transactions.copy()\n",
    "    Redo = set()\n",
    "    print(\"Undo: \", Undo)\n",
    "    print(\"Redo: \", Redo)\n",
    "    \n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(header)\n",
    "    \n",
    "    print(\"=============== get Undo & Redo set =====================\")\n",
    "    count = 1\n",
    "    with open(schedule_path,'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if count in range(line_last_checkpoint + 1, line_number):\n",
    "                print(line, end = \"\")\n",
    "                if line[0] == \"<\":\n",
    "                    if line[1:3] not in Undo:\n",
    "                        Undo.add(line[1:3])\n",
    "                    elif line[5:5+6] == \"commit\":\n",
    "                        Undo.remove(line[1:3])\n",
    "                        Redo.add(line[1:3])\n",
    "                    elif line[5:5+8] == \"rollback\":\n",
    "                        Undo.remove(line[1:3])\n",
    "                        Redo.add(line[1:3])\n",
    "                print(\"Undo: \", Undo)\n",
    "                print(\"Redo: \", Redo)\n",
    "            if count == line_number:\n",
    "                break\n",
    "            count += 1 \n",
    "    print(\"============== get Undo & Redo end ======================\")\n",
    "    \n",
    "    \n",
    "    redo_log = \"redo \"\n",
    "    undo_log = \"undo \"\n",
    "    print(\"redo \",end=\"\")\n",
    "    i = 0\n",
    "    for tran in sorted(Redo):\n",
    "        if( i != len(Redo) - 1):\n",
    "            print(\"<{0}>, \".format(tran),end=\"\")\n",
    "            redo_log +=  \"<{0}>, \".format(tran)\n",
    "        else:\n",
    "            print(\"<\",tran,\">\")\n",
    "            redo_log +=  \"<\"+tran+\">\\n\"\n",
    "        i += 1\n",
    "    if(len(Redo) == 0):\n",
    "        redo_log += \"\\n\"\n",
    "        \n",
    "    j = 0\n",
    "    for tran in sorted(Undo):\n",
    "        if( j != len(Undo) - 1):\n",
    "            print(\"<{0}>, \".format(tran),end=\"\")\n",
    "            undo_log +=  \"<{0}>, \".format(tran)\n",
    "        else:\n",
    "            print(\"<\",tran,\">\")\n",
    "            undo_log +=  \"<\"+tran+\">\\n\"\n",
    "        j += 1\n",
    "    if(len(Undo) == 0):\n",
    "        undo_log += \"\\n\"\n",
    "        \n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(redo_log)\n",
    "        f.write(undo_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -run sample_schedules.sched\n",
    "# read_file(dirpath + \"/search.txt\")\n",
    "# read_file(dirpath + \"/proj2.log\")\n",
    "# read_file(dirpath + \"/sample_schedules.sched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "PW = \"1360\"\n",
    "DBASE = \"test\"\n",
    "WIKI = \"wiki\"\n",
    "LINK = \"link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = pymysql.connect(host = 'localhost', \n",
    "                            user = 'root',\n",
    "                            password = PW,\n",
    "                            db = DBASE,\n",
    "                            charset = 'utf8mb4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con = pymysql.connect(host = 'localhost', \n",
    "#                             user = 'root',\n",
    "#                             password = PW,\n",
    "#                             db = DBASE,\n",
    "#                             charset = 'utf8mb4')\n",
    "with con.cursor() as cursor:\n",
    "#         # Create a new record\n",
    "#         sql = \"INSERT INTO `users` (`email`, `password`) VALUES (%s, %s)\"\n",
    "#         cursor.execute(sql, ('webmaster@python.org', 'very-secret'))\n",
    "    print(pd.read_sql(\"show tables\",con))\n",
    "    # connection is not autocommit by default. So you must commit to save\n",
    "    # your changes.\n",
    "    print(\"start trans: \", cursor.execute(\"start transaction\"))\n",
    "    cursor.execute(\"INSERT INTO test (id, name) VALUES (4, 'saewook')\")\n",
    "#     cursor.execute(\"INSERT INTO test (id, name) VALUES (3, 'gawon')\")\n",
    "#     cursor.execute(\"INSERT INTO test (id, name) VALUES (5, 'bob')\")\n",
    "#     cursor.execute(\"delete from test where id = 4\")\n",
    "    print(\"num of record:\", cursor.execute(\"select * from test\"))\n",
    "    result = cursor.fetchall()\n",
    "    print(result)\n",
    "#     print(\"commit: \", cursor.execute(\"commit\"))\n",
    "#     con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con2 = pymysql.connect(host = 'localhost', \n",
    "                            user = 'root',\n",
    "                            password = PW,\n",
    "                            db = DBASE,\n",
    "                            charset = 'utf8mb4')\n",
    "with con2.cursor() as cursor:\n",
    "#         # Create a new record\n",
    "#         sql = \"INSERT INTO `users` (`email`, `password`) VALUES (%s, %s)\"\n",
    "#         cursor.execute(sql, ('webmaster@python.org', 'very-secret'))\n",
    "    print(pd.read_sql(\"show tables\",con2))\n",
    "    # connection is not autocommit by default. So you must commit to save\n",
    "    # your changes.\n",
    "    print(\"start trans: \", cursor.execute(\"start transaction\"))\n",
    "#     cursor.execute(\"INSERT INTO test (id, name) VALUES (4, 'saewook')\")\n",
    "#     cursor.execute(\"INSERT INTO test (id, name) VALUES (3, 'gawon')\")\n",
    "#     cursor.execute(\"INSERT INTO test (id, name) VALUES (5, 'bob')\")\n",
    "#     cursor.execute(\"delete from test where id = 4\")\n",
    "    print(\"num of record:\", cursor.execute(\"select * from test\"))\n",
    "    result = cursor.fetchall()\n",
    "    print(result)\n",
    "#     print(\"commit: \", cursor.execute(\"commit\"))\n",
    "#     con2.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
