{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math \n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_file_sql(con, file_name):\n",
    "    sql_file = open(file_name, 'r').readlines()\n",
    "    query = \"\"\n",
    "    i = 0\n",
    "    for sql_line in sql_file:\n",
    "        i += 1\n",
    "        sql_line = sql_line.strip()\n",
    "        query += sql_line\n",
    "        if len(query) > 0:\n",
    "            if query[len(query) - 1] == ';':\n",
    "                query = query[:len(query) - 1]\n",
    "                #print('[DEBUG ] ', query)\n",
    "                with con.cursor() as cursor:\n",
    "                    cursor.execute(query)\n",
    "                query = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(con, SQL):\n",
    "    with con.cursor() as cursor:\n",
    "        cursor.execute(SQL)\n",
    "#         result = cursor.fetchall()\n",
    "#         print(result)\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(Path):\n",
    "    with open(Path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design write search log function \n",
    "def write_search_log(num_of_search, search_query, search_result):\n",
    "    search_path = dirpath + \"/search.txt\"\n",
    "    with open(search_path,'a') as f:\n",
    "        f.write(\"search {0}\\n\".format(num_of_search))\n",
    "        f.write(\"query {0}\".format(search_query))\n",
    "        f.write(search_result+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_schedules(Path):\n",
    "    queries = []\n",
    "    schedules = []\n",
    "    with open(Path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: break\n",
    "    #         print(line)\n",
    "            queries.append(line)\n",
    "            if(line[0] == \"<\"):\n",
    "        #         print(query.split(\"> \")[1])\n",
    "                schedules.append(line.split(\"> \")[1])\n",
    "            else:\n",
    "        #         print(query)\n",
    "                schedules.append(line)\n",
    "    return schedules, queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#design write search log function \n",
    "def write_transaction_log(con, dirpath, SQL):\n",
    "    # parse transaction\n",
    "    log = \"\"\n",
    "    if \"commit\" in SQL:\n",
    "        log = \"<{0}> commit\\n\".format(re.split(\">\",SQL.split(\"<\")[1])[0])\n",
    "#         print(log)\n",
    "    elif \"DELETE\" in SQL:\n",
    "        [tran, command, table, key_value] = re.split(\"> |FROM | WHERE \",SQL.split(\"<\")[1]) \n",
    "        [key, key_v, _] = re.split(\" = |;\",key_value)\n",
    "        key_value = \"{0} = {1}\".format(key, key_v)\n",
    "#         print([tran, command, table, key, key_v])\n",
    "        with con.cursor() as cursor:\n",
    "            cursor.execute(\"select COLUMN_NAME from INFORMATION_SCHEMA.COLUMNS where TABLE_NAME= '{0}'\".format(table))\n",
    "            cols = cursor.fetchall()\n",
    "            cursor.execute(\"select * from {0} where {1} = {2}\".format(table, key, key_v))\n",
    "            old_values = cursor.fetchall()\n",
    "            log_old_values = \"\"\n",
    "            for info in old_values:\n",
    "#                 log_old_values += \"\"\n",
    "                for i, col in enumerate(cols):\n",
    "                    log_old_values  += \"{0} = {1}#\".format(col[0], info[i])\n",
    "                log_old_values += \"//\"\n",
    "        log = \"<{0}> <{1}> <{2}> <{3}>\\n\".format(tran,table,key_value, log_old_values)\n",
    "#         print(log)\n",
    "    elif \"UPDATE\" in SQL:\n",
    "        [tran, command_table, col_value, key_value] = re.split(\"> |SET | WHERE \",SQL.split(\"<\")[1])\n",
    "        [command, table, _] = command_table.split(\" \")\n",
    "        [col, new_v] = re.split(\" = \",col_value)\n",
    "        [key, key_v, _] = re.split(\" = |;\",key_value)\n",
    "        new_value = \"{0} = {1}\".format(col, new_v)\n",
    "        key_value = \"{0} = {1}\".format(key, key_v)\n",
    "#         print([tran, command, table, col_value, key_value])\n",
    "        old_value = \"\"\n",
    "        with con.cursor() as cursor:\n",
    "            cursor.execute(\"select {0} from {1} where {2} = {3}\".format(col, table, key, key_v))\n",
    "            result = cursor.fetchall()\n",
    "            old_v = \"\"\n",
    "            if(len(result) != 0):\n",
    "                old_v = result[0][0]\n",
    "            old_value = \"{0} = {1}\".format(col, old_v)\n",
    "        log = \"<{0}> <{1}> <{2}> <{3}> <{4}>\\n\".format(tran,table,key_value, old_value, new_value)\n",
    "#         print(log)\n",
    "#         execute_sql(con, SQL[5:])\n",
    "#         print(log)\n",
    "    elif \"rollback\" in SQL:\n",
    "        log = \"<{0}> abort\\n\".format(re.split(\">\",SQL.split(\"<\")[1])[0])\n",
    "#         print(log)\n",
    "    log_path = dirpath + \"/prj2.log\"\n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_checkpoint(dirpath, active_transactions):\n",
    "    log_path = dirpath + \"/prj2.log\"    \n",
    "    log = \"checkpoint \"\n",
    "    print(\"checkpoint \",end=\"\")\n",
    "    i = 0\n",
    "    for tran in sorted(active_transactions):\n",
    "        if( i != len(active_transactions) - 1):\n",
    "            print(\"<{0}>, \".format(tran),end=\"\")\n",
    "            log +=  \"<{0}>, \".format(tran)\n",
    "        else:\n",
    "            print(\"<\",tran,\">\")\n",
    "            log +=  \"<\"+tran+\">\\n\"\n",
    "        i += 1\n",
    "    if(len(active_transactions) == 0):\n",
    "        log += \"\\n\"\n",
    "    \n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_based_recovery(dirpath, line_number, active_transactions, line_last_checkpoint):\n",
    "    log_path = dirpath + \"/recovery.txt\"\n",
    "    schedule_path = dirpath + \"/prj2.sched\"\n",
    "    \n",
    "    header = \"recover {0}\\n\".format(line_number)\n",
    "    print(header)\n",
    "    print(\"last check point line: \",line_last_checkpoint)\n",
    "    print(active_transactions)\n",
    "    \n",
    "    Undo = active_transactions.copy()\n",
    "    Redo = set()\n",
    "    print(\"Undo: \", Undo)\n",
    "    print(\"Redo: \", Redo)\n",
    "    \n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(header)\n",
    "    \n",
    "    print(\"=============== get Undo & Redo set =====================\")\n",
    "    count = 1\n",
    "    with open(schedule_path,'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if count in range(line_last_checkpoint + 1, line_number):\n",
    "                print(line, end = \"\")\n",
    "                if line[0] == \"<\":\n",
    "                    tran = re.split(\">\",line.split(\"<\")[1])[0]\n",
    "                    if tran not in Undo:\n",
    "                        Undo.add(tran)\n",
    "#                     elif line[5:5+6] == \"commit\":\n",
    "                    elif \"commit\" in line:\n",
    "                        Undo.remove(tran)\n",
    "                        Redo.add(tran)\n",
    "#                     elif line[5:5+8] == \"rollback\":\n",
    "                    elif \"rollback\" in line:\n",
    "                        Undo.remove(tran)\n",
    "                        Redo.add(tran)\n",
    "                print(\"Undo: \", Undo)\n",
    "                print(\"Redo: \", Redo)\n",
    "            if count == line_number:\n",
    "                break\n",
    "            count += 1 \n",
    "    print(\"============== get Undo & Redo end ======================\")\n",
    "    \n",
    "    redo_log = \"redo \"\n",
    "    undo_log = \"undo \"\n",
    "    print(\"redo \",end=\"\")\n",
    "    i = 0\n",
    "    for tran in sorted(Redo):\n",
    "        if( i != len(Redo) - 1):\n",
    "            print(\"<{0}>, \".format(tran),end=\"\")\n",
    "            redo_log +=  \"<{0}>, \".format(tran)\n",
    "        else:\n",
    "            print(\"<\",tran,\">\")\n",
    "            redo_log +=  \"<\"+tran+\">\\n\"\n",
    "        i += 1\n",
    "    if(len(Redo) == 0):\n",
    "        redo_log += \"\\n\"\n",
    "        \n",
    "    j = 0\n",
    "    for tran in sorted(Undo):\n",
    "        if( j != len(Undo) - 1):\n",
    "            print(\"<{0}>, \".format(tran),end=\"\")\n",
    "            undo_log +=  \"<{0}>, \".format(tran)\n",
    "        else:\n",
    "            print(\"<\",tran,\">\")\n",
    "            undo_log +=  \"<\"+tran+\">\\n\"\n",
    "        j += 1\n",
    "    if(len(Undo) == 0):\n",
    "        undo_log += \"\\n\"\n",
    "        \n",
    "    with open(log_path,'a') as f:\n",
    "        f.write(redo_log)\n",
    "        f.write(undo_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tran_commit(con, tran, dirpath, log_transaction_start_line):\n",
    "    log_path = dirpath + \"/prj2.log\"\n",
    "    R = None\n",
    "    N_idx = None\n",
    "    print(log_transaction_start_line)\n",
    "    with con.cursor() as curosr:       \n",
    "        print(\"======================= commit start ==========================\")\n",
    "        count = 1\n",
    "        with open(log_path,'r') as f:\n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                if log_transaction_start_line[tran] <= count:\n",
    "                    tran_line = re.split(\">\",line.split(\"<\")[1])[0]\n",
    "    #                 print(\"debug: \", count, tran_line)\n",
    "                    if tran_line == tran:\n",
    "                        parse_list = re.split(\"<|> <|>\",line)\n",
    "                        if len(parse_list) == 6: # DELETE\n",
    "                            [_, tran, table, key_value, deleted_list, _] = parse_list\n",
    "                            [key, key_v] = re.split(\" = \", key_value)\n",
    "                            sql = \"DELETE FROM {0} WHERE {1} = {2}\".format(table, key, key_v)\n",
    "                            print(sql)\n",
    "                            execute_sql(con, sql)\n",
    "                        elif len(parse_list) == 7: # UPDATE\n",
    "                            [_, tran, table, key_value, old_value, new_value,_] = parse_list\n",
    "                            [key, key_v] = re.split(\" = \", key_value)\n",
    "                            [col, new_v] = re.split(\" = \", new_value)\n",
    "                            sql = \"UPDATE {0} SET {1} = {2} WHERE {3} = {4}\".format(table, col, new_v, key, key_v)\n",
    "                            print(sql)\n",
    "                            execute_sql(con,sql) \n",
    "                        elif \"rollback\" in line:\n",
    "                            # =============== rollback transaction ======================\n",
    "                            execute_tran_rollback(con, tran, dirpath, count)\n",
    "                        \n",
    "                        if \"commit\" in line:\n",
    "                            break\n",
    "                count += 1\n",
    "                \n",
    "        create_invertedindex(con)\n",
    "        [R, N_idx] = get_pagerank_score(con)\n",
    "        con.commit()\n",
    "        print(\"======================= commit end ==========================\")\n",
    "        \n",
    "    return R, N_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tran_rollback(con, tran, dirpath, log_line_number):\n",
    "    log_path = dirpath + \"/prj2.log\"\n",
    "    R = None\n",
    "    N_idx = None\n",
    "    print(log_transaction_start_line)\n",
    "    with con.cursor() as curosr:       \n",
    "        print(\"======================= rollback start ==========================\")\n",
    "        count = log_line_number\n",
    "        for line in reversed(open(\"prj2.log\").readlines()):\n",
    "            log_line = line.rstrip()\n",
    "            tran_line = re.split(\">\",log_line.split(\"<\")[1])[0]\n",
    "            if tran_line == tran:\n",
    "                parse_list = re.split(\"<|> <|>\",log_line)\n",
    "                if len(parse_list) == 6: # DELETE\n",
    "                    [_, tran, table, key_value, deleted_list, _] = parse_list\n",
    "                    [key, key_v] = re.split(\" = \", key_value)\n",
    "                    old_records = re.split(\"//\", deleted_list)\n",
    "                    print(count) # rollback 해야할 log line \n",
    "                    for i in range(len(old_records) - 1): # insert 해야할 recored 수 \n",
    "                        insert_values = re.split(\"#\", old_records[i])\n",
    "                        sql = \"insert into {0}\".format(table) \n",
    "                        col = \"(\"\n",
    "                        val = \"(\"\n",
    "                        val_length = len(insert_values)\n",
    "                        for i, value in enumerate(insert_values):\n",
    "                            if (i < len(insert_values) - 1):\n",
    "                                [c, v] = re.split(\" = \", value)\n",
    "                                if i == len(insert_values) - 2:\n",
    "                                    col += \"{0})\".format(c)\n",
    "                                    val += \"{0})\".format(v)\n",
    "                                else:\n",
    "                                    col += \"{0},\".format(c)\n",
    "                                    val += \"{0},\".format(v)\n",
    "                        sql += \" {0} values {1}\".format(col, val)\n",
    "                        print(count, sql)\n",
    "                        execute_sql(con, sql)\n",
    "#                         print(sql)\n",
    "#                     sql = \"DELETE FROM {0} WHERE {1} = {2}\".format(table, key, key_v)\n",
    "#                     print(sql)\n",
    "#                     execute_sql(con, sql)\n",
    "                elif len(parse_list) == 7: # UPDATE\n",
    "                    [_, tran, table, key_value, old_value, new_value,_] = parse_list\n",
    "                    [key, key_v] = re.split(\" = \", key_value)\n",
    "                    [col, new_v] = re.split(\" = \", new_value)\n",
    "                    [col, old_v] = re.split(\" = \", old_value)\n",
    "                    old_v = \"{}\".format(old_v)\n",
    "                    sql = \"UPDATE {0} SET {1} = %s WHERE {2} = {3}\".format(table, col, key, key_v)\n",
    "                    print(count, sql)\n",
    "#                     execute_sql(con)\n",
    "                    with con.cursor() as cursor:\n",
    "                        cursor.execute(sql, old_v)\n",
    "                if \"start\" in log_line:\n",
    "                    break\n",
    "#                 print(count, line.rstrip())\n",
    "            count -= 1  \n",
    "        con.commit()\n",
    "        print(\"======================= rollback end ==========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFscore(Nd, Ndt, Nt):\n",
    "        return math.log1p(Ndt/Nd)*(1/Nt)\n",
    "    \n",
    "def SortCriteria(item):\n",
    "        return item[2]*item[3]\n",
    "       \n",
    "def print_search_query(con, querys, R, N_idx):\n",
    "    TFIDFs_idx = {}\n",
    "    TFIDFs_idx_inverse = {}\n",
    "    TFIDFs = []\n",
    "    iteration = 0\n",
    "    for query in querys:\n",
    "        #######################################################################\n",
    "        # # TF-IDF Score calculation for each word \n",
    "        #######################################################################\n",
    "        sql = \"select sum(freq),id from InvertedIndex where id in (select id from InvertedIndex where term = %s) group by id order by id\"\n",
    "        NdInfo = None\n",
    "        with con.cursor() as cursor:\n",
    "            cursor.execute(sql,query)\n",
    "            NdInfo = cursor.fetchall()\n",
    "\n",
    "        sql = \"select freq, id, term from InvertedIndex where term = %s order by id\"\n",
    "        NdtInfo = None\n",
    "        with con.cursor() as cursor:\n",
    "            cursor.execute(sql,query)\n",
    "            NdtInfo = cursor.fetchall()\n",
    "\n",
    "        sql = \"select count(*) from InvertedIndex where term = %s\"\n",
    "        Nt = None\n",
    "        with con.cursor() as cursor:   \n",
    "            cursor.execute(sql,query)\n",
    "            Nt = cursor.fetchall()[0][0]\n",
    "\n",
    "        TFIDF = []\n",
    "        for i in range(len(NdtInfo)):\n",
    "            if NdInfo[i][1] == NdtInfo[i][1]:\n",
    "                Nd = NdInfo[i][0]\n",
    "                Ndt = NdtInfo[i][0]\n",
    "                id = NdInfo[i][1]\n",
    "        #         print(id,\" \",math.log1p(Ndt/Nd)*(1/Nt))\n",
    "        #         TFIDF.append((id, math.log1p(Ndt/Nd)*(1/Nt)))\n",
    "                TFIDF.append((id, TFIDFscore(Nd, Ndt, Nt)))\n",
    "        TFIDFs.append(TFIDF)\n",
    "        TFIDFs_idx[query] = iteration\n",
    "        TFIDFs_idx_inverse[iteration] = query\n",
    "        iteration = iteration + 1\n",
    "\n",
    "    # Get title dictionary     \n",
    "    sql = \"select id,title from wiki order by id\"\n",
    "    id_title = None\n",
    "    with con.cursor() as cursor:\n",
    "        cursor.execute(sql)\n",
    "        id_title = cursor.fetchall()\n",
    "    id_title_dictionary = {}\n",
    "    for _, idtitle in enumerate(id_title):\n",
    "        v_id = idtitle[0]\n",
    "        v_title = idtitle[1]\n",
    "        id_title_dictionary[v_id] = v_title\n",
    "\n",
    "    # Union TFIDFs\n",
    "    TFIDFsetlist = []      \n",
    "    for query in querys:\n",
    "        temp = np.array(TFIDFs[TFIDFs_idx[query]])\n",
    "        if len(temp) == 0:\n",
    "            idsForOneQuery = temp.astype(int)\n",
    "        else:\n",
    "            idsForOneQuery = temp[...,0].astype(int)\n",
    "        TFIDFsetlist.append(set(idsForOneQuery))\n",
    "\n",
    "    UnionId = []\n",
    "    UnionId_idx = {}\n",
    "    UnionId_idx_inverse = {}\n",
    "    for TFIDFset in TFIDFsetlist:\n",
    "        UnionId = list(set(UnionId)|set(TFIDFset))\n",
    "\n",
    "    UnionId.sort()\n",
    "    iteration = 0;\n",
    "    for idvalue in UnionId:\n",
    "        UnionId_idx[idvalue] = iteration\n",
    "        UnionId_idx_inverse[iteration] = idvalue\n",
    "        iteration = iteration + 1\n",
    "\n",
    "    # Get TFIDF Score Matrix SM\n",
    "    SM = np.zeros((len(querys), len(UnionId)))\n",
    "    for row in range(len(querys)):\n",
    "        LenOfcols = len(TFIDFs[row])\n",
    "    #     print(LenOfcols)\n",
    "        for j in range(LenOfcols): # j means each query's index of id\n",
    "    #         print(TFIDFs[row][j][0], TFIDFs[0][j][1], TFIDFs[0][j][2])  # \n",
    "            col = UnionId_idx[TFIDFs[row][j][0]]\n",
    "    #         title = TFIDFs[row][j][1]\n",
    "            score = TFIDFs[row][j][1]\n",
    "            SM[row][col] = score\n",
    "    # SM\n",
    "    #######################################################################\n",
    "    # # Get Top K list\n",
    "    # # Input: TFIDF Score Matrix SM, PageRank list R\n",
    "    # # Output: QAList(sorted order by TFIDF score * PageRank score)\n",
    "    #######################################################################\n",
    "    QAList = []\n",
    "    for it in UnionId_idx:\n",
    "        Id = it\n",
    "        UnionTFIDFScore = SM[...,UnionId_idx[it]].sum()\n",
    "        Uniontitle = id_title_dictionary[it] \n",
    "        if Id in N_idx:\n",
    "            PrankScore = R[N_idx[Id]][0]\n",
    "        else:\n",
    "            print(\"the link does not exist\")\n",
    "            PrankScore = 0\n",
    "        QAList.append((Id, Uniontitle, UnionTFIDFScore, PrankScore))\n",
    "\n",
    "    QAList.sort(key= SortCriteria, reverse= True)\n",
    "\n",
    "    strFormat = '%-10s%-60s%-20s%-20s\\n'\n",
    "    strOut = strFormat % ('id', 'title', 'TF-IDF', 'PageRank')\n",
    "    iteration = 0;\n",
    "    for ans in QAList:\n",
    "        if iteration < 10:\n",
    "#             print(ans[0], ans[1], format(ans[2],\"10.2e\"), format(ans[3],\"10.2e\"))\n",
    "            strOut += strFormat %(ans[0], ans[1], format(ans[2],\"10.2e\"), format(ans[3],\"10.2e\"))\n",
    "        else:\n",
    "           break\n",
    "        iteration = iteration + 1\n",
    "    print(strOut)\n",
    "    \n",
    "    return strOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invertedindex(con):\n",
    "    with con.cursor() as cursor:\n",
    "        cursor.execute(\"drop table if exists InvertedIndex\")\n",
    "        sql = \"select * from wiki\"\n",
    "        cursor.execute(sql)\n",
    "        result_wiki = cursor.fetchall()\n",
    "         # Create inverted index table \n",
    "        sql = \"create table InvertedIndex (term varchar(255) not null, id int(11) not null, freq int(11) not null)\"\n",
    "        cursor.execute(sql)\n",
    "        # # Clean inverted index table\n",
    "        # sql = \"delete from InvertedIndex\" \n",
    "        # cursor.execute(sql)\n",
    "\n",
    "        # insertion from wiki table \n",
    "        sql = \"insert into InvertedIndex (term,id,freq) values (%s,%s,%s)\"\n",
    "        for Doc in result_wiki:\n",
    "        #     print(type(Doc)) #tuple (id, doc_name, doc_script)\n",
    "            tokens = nltk.word_tokenize(Doc[2].lower())\n",
    "            fdist = nltk.FreqDist(tokens) # dictionary {term:freq, ... }\n",
    "            for term, freq in fdist.items():\n",
    "                cursor.execute(sql,(term, Doc[0], freq))\n",
    "        #     print(fdist)\n",
    "        con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagerank_score(con):\n",
    "    R = None\n",
    "    N_idx = None\n",
    "    with con.cursor() as cursor:\n",
    "        sql = \"select id_from, count(*) as outgoing from link group by id_from order by id_from\"\n",
    "        cursor.execute(sql)\n",
    "        FromNiInfo = cursor.fetchall()\n",
    "\n",
    "        Ni_dict = {}\n",
    "        for idx, FromNi in enumerate(FromNiInfo):\n",
    "        #     print(FromNi[0], FromNi[1])\n",
    "            id_from = FromNi[0]\n",
    "            Ni = FromNi[1]\n",
    "            Ni_dict[id_from] = Ni\n",
    "\n",
    "        # Get N and id_all(sorted order) \n",
    "        sql = \"select distinct id_from from link order by id_from\"\n",
    "        cursor.execute(sql)\n",
    "        SetOfFromInfo = cursor.fetchall()\n",
    "\n",
    "        sql = \"select distinct id_to from link order by id_to\"\n",
    "        cursor.execute(sql)\n",
    "        SetOfToInfo = cursor.fetchall()\n",
    "\n",
    "        SetOfFrom = set()\n",
    "        SetOfTo = set()\n",
    "\n",
    "        for idx, From in enumerate(SetOfFromInfo):\n",
    "        #     print(type(From[0]))\n",
    "            SetOfFrom.add(From[0])\n",
    "\n",
    "        for idx, To in enumerate(SetOfToInfo):\n",
    "        #     print(type(From[0]))\n",
    "            SetOfTo.add(To[0])\n",
    "\n",
    "        id_all = sorted(SetOfFrom.union(SetOfTo))\n",
    "        N = len(id_all)\n",
    "\n",
    "        # Get N_idx and N_idx_inverse; it means dictionary[Doc.id] = index of Transition Matrix or State Matrix    \n",
    "        N_idx = {}\n",
    "        N_idx_inverse = {}\n",
    "        for idInfo, idx in enumerate(id_all):\n",
    "        #     print(idx, idInfo)\n",
    "            N_idx[idx] = idInfo\n",
    "            N_idx_inverse[idInfo] = idx\n",
    "\n",
    "        # Get SateMatrix S; check whether existing from j to i link  \n",
    "        S = np.zeros((N,N)) # from j to i info : S[i][j]\n",
    "        sql = \"select * from link order by id_from\"\n",
    "        cursor.execute(sql)\n",
    "        FromToInfo = np.array(cursor.fetchall())\n",
    "        for fromto in FromToInfo:\n",
    "            id_to = fromto[1]\n",
    "            id_from = fromto[0]\n",
    "            S[N_idx[id_to]][N_idx[id_from]] = 1\n",
    "\n",
    "        # Get Transition Matrix M and Score Vector R \n",
    "        M = np.zeros((N,N)) # from j to i info : M[i][j]\n",
    "        for _, id_from in enumerate(sorted(SetOfFrom)):\n",
    "            for _, id_to in enumerate(id_all):\n",
    "                # if link id_from to id_to exists\n",
    "                if S[N_idx[id_to]][N_idx[id_from]] != 0:\n",
    "                    M[N_idx[id_to]][N_idx[id_from]] = 1/Ni_dict[id_from]\n",
    "\n",
    "        # PageRank Algorithm\n",
    "        # Input: Station Matrix S, Transition Matrix T, RankVector R \n",
    "        # Output: updated RankVector R \n",
    "        delta = 0.15\n",
    "        elipslion = 1e-8\n",
    "        # R = np.ones((N,1))*(1/N)\n",
    "        R = np.ones((N,1))\n",
    "        K = np.ones((N,1))*(delta/N)\n",
    "        # R = delta * np.matmul(M,prevR) + K\n",
    "        iteration = 0\n",
    "        distance = 100\n",
    "        while distance > elipslion:\n",
    "        #     print(\"iteration\", iteration, \"...\")\n",
    "            prevR = R\n",
    "            R = delta * np.matmul(M,R) + K\n",
    "            iteration = iteration + 1\n",
    "            distance = np.linalg.norm(R-prevR)\n",
    "        #     print(\"distance = \",np.linalg.norm(R-prevR))\n",
    "    return R,N_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# # Connect to mysql\n",
    "# # Output: con obj, cursor obj\n",
    "#######################################################################\n",
    "# IP = \"s.snu.ac.kr\"\n",
    "# ID = \"ADB2018_26190\"\n",
    "# PW = \"ADB2018_26190\"\n",
    "# DBase = \"ADB2018_26190\"\n",
    "PW = \"1360\"\n",
    "DBASE = \"test\"\n",
    "WIKI = \"wiki\"\n",
    "LINK = \"link\"\n",
    "\n",
    "con = pymysql.connect(host = 'localhost', \n",
    "                            user = 'root',\n",
    "                            password = PW,\n",
    "                            db = DBASE,\n",
    "                            charset = 'utf8mb4')\n",
    "cursor = con.cursor()\n",
    "dirpath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 올바른 wiki, link, invertedindex가 있어야 TFIDF와 pagerank를 올바르게 계산될 수 있다.\n",
    "# 초기화 시켜주는 라인들 \n",
    "execute_file_sql(con, dirpath + \"/link.sql\")\n",
    "execute_file_sql(con, dirpath + \"/wiki.sql\")\n",
    "create_invertedindex(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"building tables...\")\n",
    "# execute_file_sql(con, dirpath + \"/link.sql\")\n",
    "# execute_file_sql(con, dirpath + \"/wiki.sql\")\n",
    "\n",
    "# # Before executing this file, create wiki, link table in DBase using mysql-workbench  \n",
    "#######################################################################\n",
    "# # Make inverted index table \n",
    "# # Input: wiki table \n",
    "# # Output: InvertedIndex table(term, id, title, freq)\n",
    "#######################################################################\n",
    "sql = \"select count(*) from information_schema.tables where (table_schema = %s) and (table_name = %s)\"\n",
    "cursor.execute(sql, (DBASE, \"InvertedIndex\"))\n",
    "\n",
    "# if inverted index table does not exist, create inverted index table \n",
    "if cursor.fetchall()[0][0] == 0: \n",
    "    # Create inverted index table \n",
    "    create_invertedindex(con)\n",
    "\n",
    "print(\"ready to search...\")\n",
    "#######################################################################\n",
    "# # PageRank Score Calculation\n",
    "# # Input: link table \n",
    "# # Output: PageRank list R(id, PageRank score)\n",
    "#######################################################################\n",
    "# print(\"calculating pagerank score ...\")\n",
    "# Get Ni for each\n",
    "[R, N_idx] = get_pagerank_score(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# # TF-IDF Score calculation\n",
    "# # Input: InvertedIndex table, wiki table, query(string)\n",
    "# # Output: TF-IDF list TFIDF(id, title, TF-IDF score) \n",
    "#######################################################################\n",
    "while True:\n",
    "    # Get input from console\n",
    "    print(\"2018-26190>\",end = '')\n",
    "    query = input()\n",
    "    if query == \"exit()\":\n",
    "        break\n",
    "    elif query[0] == \"-\": # -run *.sched # -run prj2.sched\n",
    "        schedules_path = dirpath + \"/\"+query.split(\"-run \")[1]\n",
    "        if os.path.isfile(schedules_path):\n",
    "            [schedules, queries] = parse_schedules(schedules_path)\n",
    "            # initialize search.txt\n",
    "            num_of_search = 0\n",
    "            open(dirpath+'/search.txt', 'w').close()\n",
    "            open(dirpath+'/prj2.log', 'w').close()\n",
    "            open(dirpath+'/recovery.txt', 'w').close()\n",
    "            active_transactions = set()\n",
    "            line_number = 1\n",
    "            line_last_checkpoint = 1\n",
    "            log_transaction_start_line = {}\n",
    "            log_line_number = 1\n",
    "            # execute schedules =================== 6 functions should be implemented \n",
    "            for schedule in queries:\n",
    "                print(schedule)\n",
    "                if schedule[:6] == \"search\":\n",
    "                    search_query = schedule.split(\"search \")[1]\n",
    "                    print(search_query)\n",
    "                    search_result = print_search_query(con, search_query.lower().split(), R, N_idx)\n",
    "                    write_search_log(num_of_search, search_query, search_result)\n",
    "                    num_of_search += 1   \n",
    "                elif schedule[0] == \"<\":\n",
    "                    tran = re.split(\">\",schedule.split(\"<\")[1])[0]\n",
    "                    # active_transaction update\n",
    "                    if tran not in active_transactions:\n",
    "                        active_transactions.add(tran)\n",
    "                        print(\"<{0}> start\\n\".format(tran))\n",
    "                        with open(dirpath+\"/prj2.log\",'a') as f:\n",
    "                            f.write(\"<{0}> start\\n\".format(tran))\n",
    "                            log_transaction_start_line[tran] = log_line_number\n",
    "                            log_line_number += 1\n",
    "                    # write log <Transaction ID> [SQL]  \n",
    "#                     print(schedule)\n",
    "                    write_transaction_log(con, dirpath, schedule)\n",
    "#                     log_line_number += 1    \n",
    "                    if \"commit\" in schedule:\n",
    "                        active_transactions.remove(tran)\n",
    "                        # =============== commit transaction =======================\n",
    "#                         execute_tran_commit(con, tran, dirpath, log_transaction_start_line)\n",
    "                        [R, N_idx] = execute_tran_commit(con, tran, dirpath, log_transaction_start_line)\n",
    "                        del log_transaction_start_line[tran]\n",
    "                    elif \"rollback\" in schedule:\n",
    "                        active_transactions.remove(tran)\n",
    "#                         # =============== rollback transaction ======================\n",
    "#                         execute_tran_rollback(con, tran, dirpath, log_line_number)\n",
    "                        del log_transaction_start_line[tran]\n",
    "                        \n",
    "                    log_line_number += 1    \n",
    "                # write checkpoint log  \n",
    "                elif schedule[:10] == \"checkpoint\":\n",
    "                    print(schedule)\n",
    "                    write_checkpoint(dirpath, sorted(active_transactions))\n",
    "                    log_line_number += 1\n",
    "                    line_last_checkpoint = line_number\n",
    "                    print(log_transaction_start_line)\n",
    "                # system failure -recover\n",
    "                elif schedule[:6] == \"system\":\n",
    "                    print(schedule)\n",
    "                    log_based_recovery(dirpath, line_number, active_transactions, line_last_checkpoint)\n",
    "                line_number += 1\n",
    "        else:\n",
    "            print(\"such file is not exist\")\n",
    "    else:\n",
    "        querys = query.lower().split()\n",
    "        print_search_query(con, querys, R, N_idx)\n",
    "    \n",
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soju 가 검색이 되도록 wiki table 또는 link 테이블을 바꿨을 경우, inverted index와 pagerank를 새로 계산한다.\n",
    "# update wiki table \n",
    "# execute_sql(con, \"UPDATE wiki SET text = 'Soju Manbyungman Huso' WHERE id = 11374610;\")\n",
    "execute_sql(con, \"UPDATE wiki SET text = 'We are not a team. This is a competition.' WHERE id = 24031233;\")\n",
    "#update invertedindex and pagerank score and N_idx \n",
    "create_invertedindex(con)\n",
    "[R, N_idx] = get_pagerank_score(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with con.cursor() as cursor:\n",
    "    cursor.execute(\"select * from wiki where id = 6241635\")\n",
    "    result =cursor.fetchall()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
